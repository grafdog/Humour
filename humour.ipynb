{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/outputs /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "nXVr0xwGET7n"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install simplet5"
      ],
      "metadata": {
        "id": "2YsrqDCG0JnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "5RoftOL7HvJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import openai\n",
        "# from simplet5 import SimpleT5\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "pt3kqGLzz9ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "df_trfun = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/train_funlines.csv')\n",
        "df_dev = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/dev.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/test.csv')"
      ],
      "metadata": {
        "id": "ZddbUSPsv5a0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = ''\n",
        "\n",
        "def score(truth, pred):\n",
        "    assert(sorted(truth.id) == sorted(pred.id)),\"ID's do not match\"\n",
        "\n",
        "    data = pd.merge(truth, pred)\n",
        "    rmse = np.sqrt(np.mean((data['meanGrade'] - data['pred'])**2))\n",
        "\n",
        "    return rmse\n",
        "\n",
        "\n",
        "def t5_prep(df):\n",
        "    source = []\n",
        "    target = []\n",
        "    for i in range(len(df)):\n",
        "        source.append(prefix + re.sub('<.*?>',  df.edit[i],    df.original[i]))\n",
        "        target.append(str(df.meanGrade[i]))\n",
        "    df_new = pd.DataFrame(list(zip(source, target)),\n",
        "                          columns =['source_text', 'target_text'])\n",
        "    return df_new\n",
        "\n",
        "df_train['meanGrade'] = round(df_train['meanGrade']*2)/2\n",
        "df_trfun['meanGrade'] = round(df_trfun['meanGrade']*2)/2\n",
        "\n",
        "df_dev = t5_prep(df_dev)\n",
        "df_test = t5_prep(df_test)\n",
        "\n",
        "df_train = t5_prep(df_train)\n",
        "# df_trfun = t5_prep(df_trfun)\n",
        "# df_train = pd.concat([df_train, df_trfun])\n",
        "# df_train = df_train.reset_index(drop=True)\n",
        "model = SimpleT5()\n",
        "\n"
      ],
      "metadata": {
        "id": "YYCDc74p0f4x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the T5 model"
      ],
      "metadata": {
        "id": "UDYhALBDpIow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = input('Choose a path for output files (\"/content/outputs\" by default)')\n",
        "if directory == '':\n",
        "    directory = '/content/outputs'\n",
        "\n",
        "# 't5-base' can be changed to 't5-small'\n",
        "model.from_pretrained('t5','t5-base')\n",
        "model.train(train_df = df_train,\n",
        "            eval_df = df_dev, \n",
        "            source_max_token_len=512, \n",
        "            target_max_token_len=128, \n",
        "            batch_size=8, \n",
        "            max_epochs=15, \n",
        "            use_gpu=True,\n",
        "            outputdir = directory,\n",
        "            early_stopping_patience_epochs = 1\n",
        "           )\n",
        "\n",
        "path_list = [directory + '/' + x for x in os.listdir(directory)]\n",
        "path_list.sort()"
      ],
      "metadata": {
        "id": "L-lIRHZEv1kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_list = ['/content/drive/MyDrive/outputs/simplet5-epoch-0-train-loss-0.7055-val-loss-2.4302',\n",
        "             '/content/drive/MyDrive/outputs/simplet5-epoch-1-train-loss-0.5976-val-loss-2.6281']"
      ],
      "metadata": {
        "id": "bTdfxAyuMvcv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the train model to get predictions (humour ratings)"
      ],
      "metadata": {
        "id": "Oj5UsY6jpP1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path_list = [directory + '/' + x for x in os.listdir(directory)]\n",
        "# path_list.sort()\n",
        "\n",
        "rmse_list = []\n",
        "num_list = []\n",
        "df_all_pred = pd.DataFrame()\n",
        "for i in range(len(path_list)):\n",
        "  model.load_model(\"t5\", path_list[i], use_gpu=False)\n",
        "  num_list.append(i)\n",
        "  \n",
        "  predictions = []\n",
        "  for k in df_test.source_text:\n",
        "    predictions.append(model.predict(k))\n",
        "\n",
        "  for k in range(len(predictions)):\n",
        "    predictions[k] = float(predictions[k][0])\n",
        "\n",
        "  test = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "  df_pred = pd.DataFrame()\n",
        "  df_pred['id'] = test['id']\n",
        "  df_pred['pred'] = predictions\n",
        "\n",
        "  df_truth = pd.DataFrame()\n",
        "  df_truth['id'] = test['id']\n",
        "  df_truth['meanGrade'] = test['meanGrade']\n",
        "  rmse_list.append(score(df_truth, df_pred))\n",
        "  df_all_pred['epoch'+str(i)] = df_pred['pred']\n",
        "\n",
        "df_all_pred.to_csv('/content/drive/MyDrive/results/predT5-base+7class.csv')\n",
        "train_list = []\n",
        "val_list = []\n",
        "for i in path_list:\n",
        "  num = re.findall(\"[0-9][.][0-9]+\", i)\n",
        "  num = [float(x) for x in num]\n",
        "  train_list.append(num[0])\n",
        "  val_list.append(num[1])\n",
        "\n",
        "df_stats = pd.DataFrame(list(zip(num_list, train_list, val_list, rmse_list)),\n",
        "        columns =['EpochNum', 'Training_loss' , 'Validating_loss', 'RMSE'])\n",
        "df_stats.to_csv('/content/drive/MyDrive/results/PresT5-base+FL+7class.csv')"
      ],
      "metadata": {
        "id": "KzaSul6ab5yC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the most accurate completions"
      ],
      "metadata": {
        "id": "C8fRgdRPk_La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_epoch = int(df_stats.EpochNum[df_stats.RMSE == df_stats.RMSE.min()])\n",
        "t5_pred = df_all_pred.iloc[:,best_epoch] # most accurate predictions\n"
      ],
      "metadata": {
        "id": "6ExoVsb6rh1x"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT functions"
      ],
      "metadata": {
        "id": "PDyZqZm-lGCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_prep(df):\n",
        "  source = []\n",
        "  target = []\n",
        "  for i in range(len(df)):\n",
        "    prompt = (re.sub('<.*?>',  df[f'{df.columns[2]}'][i],   df[f'{df.columns[1]}'][i])+'\\n\\n###\\n\\n')\n",
        "    prompt = prompt.replace('\"', \"'\")\n",
        "\n",
        "    source.append(prompt)\n",
        "    target.append(' ' + str(df[f'{df.columns[4]}'][i])+'\\n')\n",
        "\n",
        "    df_new = pd.DataFrame(list(zip(source, target)),\n",
        "                columns =['prompt', 'completion'])\n",
        "  return df_new\n",
        "\n",
        "def gpt3_classifier(item, model, is_log=False):\n",
        "  # get the reuslt:\n",
        "  result = openai.Completion.create(model = model, \n",
        "                                    prompt=str(item),\n",
        "                                    max_tokens=5 , temperature=0)['choices'][0]['text'] \n",
        "    \n",
        "  if is_log: print('- ', item, ': ', result)\n",
        "  \n",
        "  return result"
      ],
      "metadata": {
        "id": "-f6pedKgvRCb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "df_trfun = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/train_funlines.csv')\n",
        "df_dev = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/dev.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/test.csv')"
      ],
      "metadata": {
        "id": "XmaPlUoHvbrJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['meanGrade'] = round(df_train['meanGrade'])\n",
        "df_trfun['meanGrade'] = round(df_trfun['meanGrade'])\n",
        "\n",
        "df_dev = gpt_prep(df_dev)\n",
        "df_test = gpt_prep(df_test)\n",
        "\n",
        "df_train = gpt_prep(df_train)\n",
        "df_trfun = gpt_prep(df_trfun)\n",
        "df_train = pd.concat([df_train, df_trfun])\n",
        "df_train = df_train.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "IEG1DTzaxH7N"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving data as .jsonl"
      ],
      "metadata": {
        "id": "6aLXcnuJZN1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_json('train_data.jsonl',orient = 'records', lines = True, force_ascii = False)\n",
        "df_dev.to_json('eval_data.jsonl',orient = 'records', lines = True, force_ascii = False)"
      ],
      "metadata": {
        "id": "tBbK4kVuxRXp"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading train and eval files to openai servers"
      ],
      "metadata": {
        "id": "v2XoWurwZhx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = ('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')\n",
        "openai.File.create(\n",
        "  file=open('train_data.jsonl',  encoding='utf8'),\n",
        "  purpose='fine-tune') "
      ],
      "metadata": {
        "id": "BW7s_kv02i8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = ('insert_your_api_key')\n",
        "openai.File.create(\n",
        "  file=open('eval_data.jsonl',  encoding='utf8'),\n",
        "  purpose='fine-tune') "
      ],
      "metadata": {
        "id": "_ZRc5NNzyd5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "T-7VQnOaZtMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.FineTune.create(training_file='file-4niq48fAwfapWiudZzobgeke',validation_file = 'file-lVcA546gG7umvu41PyDKbFHk', model = 'babbage')"
      ],
      "metadata": {
        "id": "ePjRPkzl2bu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking training status"
      ],
      "metadata": {
        "id": "SkFE1cqqZ1Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.FineTune.list_events(id='ft-NE8ZClj8phTpj9giPSK694eA')"
      ],
      "metadata": {
        "id": "No3y6Yfc3tRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating completions for test dataset"
      ],
      "metadata": {
        "id": "ya__NiV_Z5cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = ('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')\n",
        "answers = []\n",
        "for i in df_test['prompt']:\n",
        "    answers.append(gpt3_classifier(i, 'babbage:ft-personal-2022-12-01-00-16-16'))"
      ],
      "metadata": {
        "id": "kq-yer3l5uFU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving completions to a .csv file"
      ],
      "metadata": {
        "id": "HcdLB2LgaRl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.DataFrame()\n",
        "result['prompt'] = df_test['prompt']\n",
        "result['meanGrade'] = answers\n",
        "result.to_csv('/content/drive/MyDrive/results/gpt3_pred.csv')"
      ],
      "metadata": {
        "id": "081MryKy_6kz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_list = [float(x[1:4]) for x in answers]\n",
        "test = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "df_pred = pd.DataFrame()\n",
        "df_pred['id'] = test['id']\n",
        "df_pred['pred'] = temp_list\n",
        "\n",
        "df_truth = pd.DataFrame()\n",
        "df_truth['id'] = test['id']\n",
        "df_truth['meanGrade'] = test['meanGrade']\n",
        "score(df_truth, df_pred)"
      ],
      "metadata": {
        "id": "L4SwZuZW9oFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting a final error value for an ensemble of GPT-3 + T5"
      ],
      "metadata": {
        "id": "Mv5A3yujuj72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_bestGPT = pd.read_csv('/content/drive/MyDrive/results/GPT_best_res.csv')\n",
        "# df_bestT5 = pd.read_csv('/content/drive/MyDrive/results/T5pred_best_noround0.5.csv')\n",
        "# gpt_list = list(df_bestGPT.meanGrade)\n",
        "# t5list = list(df_bestT5.pred)\n",
        "\n",
        "gpt3_pred = temp_list\n",
        "\n",
        "\n",
        "a = []\n",
        "for i in range(len(gpt3_pred)):\n",
        "  a.append((gpt3_pred[i] + t5_pred[i]) / 2)\n",
        "\n",
        "test = pd.read_csv('/content/drive/MyDrive/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "df_pred = pd.DataFrame()\n",
        "df_pred['id'] = test['id']\n",
        "df_pred['pred'] = a\n",
        "\n",
        "df_truth = pd.DataFrame()\n",
        "df_truth['id'] = test['id']\n",
        "df_truth['meanGrade'] = test['meanGrade']\n",
        "print(score(df_truth, df_pred))"
      ],
      "metadata": {
        "id": "5fP_sJ-2tChB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}